# Chapter 03. 언어 모델(Language Model)

### 03-1 언어 모델(Language Model)이란?
- 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델

- 언어모델을 만드는 방법은 크게 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 구분할 수 있다.

- 최근에는 통계를 이용한 방법보다는 인공 신경망을 이용한 방법이 더 좋은 성능을 보여주고 있다.

- 언어 모델(Language Model)
    - 단어 시퀀스에 확률을 할당(assign)하는 일을 하는 모델

    - 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델이다.

    - 단어 시퀀스에 확률을 할당하게 하기 위해서 가장 보편적으로 사용되는 방법은 언어모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것이다.

    - 언어 모델에 -ing를 붙인 언어 모델링(Language Modeling)은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말한다.  
    -> 즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링이다.

- 단어 시퀀스의 확률 할당
    - a. 기계 번역(Machine Translation)
    ```
    P(나는 버스를 탔다) > P(나는 버스를 태운다.)
    : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.
    ```

    - b. 오타 교정(Spell Correction)
    ```
    선생님이 교실로 부리나케
    P(달려갔다) > P(잘려갔다)
    : 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.
    ```

    - c. 음성 인식(Speech Recognition)
    ```
    P(나는 메롱을 먹는다) < P(나는 메론을 먹는다.)
    : 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단.
    ```

- 주어진 이전 단어들로부터 다음 단어 예측하기
    - 언어 모델은 단어 시퀀스에 확률을 할당하는 모델이다.

    - A. 단어 시퀀스의 확률

    - B. 다음 단어 등장 확률

- 언어 모델의 간단한 직관

    - 기계는 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고  
    가장 높은 확률을 가진 단어를 선택한다.

    - 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 등장 확률을 추정하고 가장 높은 확률을 가진 단어를 선택

---
### 03-2 통계적 언어 모델(Statistical Language Model, SLM)

- 조건부 확률

- 문장에 대한 확률

- 카운트 기반의 접근

- 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)

--- 
### 03-3 N-gram 언어 모델(N-grame Language Model)

- n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종이다.

- 코퍼스에서 카운트하지 못하는 경우의 감소.

    - SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점.

    - 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다.

    - 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하자는 것

    - 이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아진다.

- N-gram

    - 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram

    - n-gram은 n개의 연속적인 단어 나열을 의미.

    - 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주.

    - 예를 들어 문장 An adorable little boy is spreading smiles가 있을 때 각 n에 대해서 n-grme을 전부 구하면
    ```
    unigrams : an, adorable, little, boy, is, spreading, smiles

    bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles

    trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles

    4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles
    ```

    - n-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram)이라고 명명.

    - 출처에 따라 유니그램, 바이그램, 트라이그램 또한 각각 1-gram, 2-gram, 3-gram이라고 하기도 한다.

- N-gram Language Model의 한계

    - 희소 문제(Sparsity Problem)
        - 문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을  
        높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재

    - n을 선택하는 것은 trade-off 문제.

        - 앞에서 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 조재

        - n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트 할 수 있는 확률은 적어지므로 희소 문제는 점점 심각

        - n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다.

        - trade-off문제로 인해 정확도를 높이려면 <span style="font-weight:bold; color:tomato">n은 최대 5를 넘게 잡아서는 안된다고 권장</span>

        - n이 성능에 영향을 주는 유명한 예제
            - 스탠퍼드 대학교의 공유자료, 월스트리트 저널에서 3,800만 개의 단어 토큰에 대하여 n-gram 언어 모델을 학습하고, 1,500만 개의   
            테스트 데이터에 대해서 테스르를 했는데, 퍼플렉서티(perplexity)는 수치가 낮을 수록 더 좋은 성능을 나타낸다.
            
            ||Unigram|Bigram|Trigram|
            |---|---|---|---|
            Perplexity|962|170|109|

            - 위 결과는 n을 1에서 2, 2에서 3으로 올릴 때마다 성능이 올라가는 것을 보여줌.

- 적용 분야(Domain)에 맞는 코퍼스의 수집

    - Domain에 따라 특정 단어들의 확률 분포는 당연히 다르다.

    - 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능 성이 높다.  
    -> 하지만, 훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라지기 때문에 언어 모델의 약점이라고 한다.

- 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)

    - N-gram Language Model의 한계점을 극복하기 위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 일반화(generalization) 방법들이 존재.

    - 그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지 못해 대안으로 인공 신경망을 이용한 언어 모델을 많이 사용함.


---
### 03-4 한국어에서의 언어 모델(Language Model for Korean Sentences)

- 한국어는 어순이 중요하지 않다.
    ```
    1. 나는 운동을 합니다. 체육관에서.
    2. 나는 체육관에서 운동을 합니다.
    3. 체육관에서 운동을 합니다.
    4. 나는 운동을 체육관에서 합니다.
    ```

    - 위 예시 처럼 단어 순서를 바꿔도 한국어는 의미가 전달 되기 때문에 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기 어렵다.

- 한국어는 교착어이다.

    - 띄어쓰기 단위인 어절 단위로 토큰화를 할 경우에는 문장에서 발생가능한 단어의 수가 굉장히 늘어남.

    - 대표적인 예로 교착어인 한국어에는 조사가 있음.

- 한국어는 띄어쓰기가 제대로 지켜지지 않는다.

    - 한국어는 띄어쓰기를 제대로 하지 않아도 의미가 전달되며, 띄어쓰기 규칙 또한 상대적으로 까다로운 언어이기 때문에 자연어 처리를 하는 것에 있어서 한국어 코퍼스는 띄어쓰기가 제대로 지켜지지 않는 경우가 많다.

---
### 03-5 펄플렉서티(Perplexity, PPL)

- Perpleity(PPL)는 텍스트 생성(Text Generation) 언어 모델의 성능 평가지표 중 하나.

- 언어 모델의 평가 방법(Evaluation metric) : PPL

    - 일반적으로 테스트 데이터셋이 충분히 신뢰할 만할 때 Perplexity 값이 낮을수록 언어 모델이 우수하다고 평가함.

- 분기 계수(Branching factor)
    - Tree 자료구조에서 branch의 개수를 의미.  
    한 가지 경우를 골라야 하는 Task에서 선택지의 개수를 뜻함.

