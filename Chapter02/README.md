# Chapter 02. 텍스트 전처리(Text preprocessing)

### 2-1 토큰화(Tokenization)

- 코퍼스 테이터가 필요에 맞게 전처리 되어 있지 않은 상태라면, 용도에 맞게 토큰화(Tokenization), 정제(cleaning), 정규화(normalization)을 진행

- 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 한다.

- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의

- 단어 토큰화(Word Tokenization)

    - 토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화(word tokenization)라고 한다.

    - 단어 외에도 단어구, 의미를 갖는 문자열로 간주되기도 함.

    - 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning)작업을 수행하는 것만으로 해결되지 않음.

    - 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생

    - 띄어쓰기 단위로 자르면 사실상 단어 토큰이 구분되는 영어와 달리 한국어는 띄어쓰기만으로 단어 토큰을 구분하기 어려움.

- 토큰화 중 생기는 선택의 순간

    - 영어의 경우 NLTK의 word_tokenize와 WordPunctTokenizer를 사용해서 어퍼스트로피를 처리할 수 있다.

- 토큰화에서 고려해야할 사항
    - 구두점이나 특수 문자를 단순 제외해서는 안된다.
        - 문장의 마침표의 경우 문장의 경계를 알 수 있는데 도움이 되므로 단어를 뽑아낼 때 마침표를 제외하지 않을 수 있다.

        - 단어 자체에 구두점을 갖고 있는 경우도 있다.(Ph.D, AT&T) 또 특수 문자의 달러나 슬래시

        - 숫자 사이에 콤마가 들어가 있는 경우(수치를 표현할때 세 자리 단위로 콤마)

    - 줄임말과 단어 내에 띄어쓰기가 있는 경우
        - 단어가 줄임말로 쓰일 때 생기는 형태 (I am -> I'm(m을 접어라고 함.))

        - New York이라는 단어나 rock 'n' roll을 보면 하나의 단어이지만 중간에 띄어쓰기가 존재  
        -> 사용 용도에 따라서 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야 할 경우도 있으므로 토큰화 작업은 저런 단어를 하나로 인식 할 수 있는 능력을 가지고 있어야 함.

    - 표준 토큰화 예제
        - 표준으로 쓰이고 있는 토큰화 방법 중 하나인 'Penn Treebank Tokenization'

        ```
        규칙 1. 하이픈으로 구성된 단어는 하나로 유지
        규칙 2. dosen't와 같이 어퍼스트로피로 '접어'가 함께하는 단어는 분리
        ```

- 문장 토큰화(Sentence Tokenization)
    - 코퍼스 내에서 문장 단위로 구분하는 작업으로 때로는 문장 분류(sentence segmentation)라고 부른다.

    - 예시
    ```
    EX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.

    EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.
    ```

    - 사용하는 코퍼스가 어떤 국적의 언어인지, 또는 해당 코퍼스 내에서 특수문자들이 어떻게 사용되고 있는지에 따라서 직접 규칙들을 정의 해 볼 수 있음.

    - NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원

    - NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않음.

    - 한국어에 대한 문장 토큰화 도구 또한 존재 KSS(Korean Sentence Splitter)

- 한국어에서의 토큰화의 어려움
    - 한국어는 영어와 달리 띄어쓰기로 토큰화를 하기 부족

    - 어절 토큰화는 한국어 NLP에서 지양되고 있다.

    - 한국어가 영어와는 다른 형태를 가지는 언어인 교착어라는 점에서 어려운 점이 있음,

    - 교착어의 특성
        - 띄어쓰기 단위가 영어처럼 독립적인 단어라면 띄어쓰기 단위로 토큰화를 하면 되겠지만 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라  
        조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리 해줘야 한다.

        - 한국어 토큰화에서는 형태소(morpheme)란 개념을 이해 해야한다.

        - 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말한다.

        - 형태소의 종류
            - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자제촐 단어가 된다.  
            체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.

            - 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간을 말한다.

    - 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.

        - 한국어 코퍼스가 뉴스 기사와 같이 띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우에 띄어쓰기가 틀렸거나 지켜지지 않는  
        코퍼스가 많이 존재

- 품사 태깅(Part-of-speech tagging)

    - 단어는 표기는 같지만 품사에 따라서 단어의 의미가 달라지기도 한다.

    - 그에 따라 단어 토큰화 과정에서 각 단어가 어떤 품사골 쓰였는지를 구분해 놓기도 하는데, 이 작업을 품사 태깅(part-of-speech tagging)이라고 한다.

    - NLTK와 KoNLPy를 통해 품사 태깅을 할 수 있다.

    - 영어 문장에서 토큰화 결과에 품사 태깅을 진행하면 PennTreebank POG Tags에서

    |명칭|설명|
    |---|---|
    |PRP|인칭 대명사|
    |VBP|동사|
    |RB|부사|
    |VBG|현재부사|
    |IN|전치사|
    |NNP|고유 명사|
    |NNS|복수형 명사|
    |CC|접속사|
    |DT|관사|

    - 한국어 형태소 토큰화의 경우 
    ```
    1) morphs : 형태소 추출
    2) pos : 품사 태깅(Part-of-speech tagging)
    3) nouns : 명사 추출
    ```

    - 각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 따라 판단하고 사용.

---
### 2-2 정제(Cleaning) and 정규화(Normalization)
- 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(Tokenization)라고 하며, 데이터를 용도에 맞게 정제(cleaning)및 정규화(normalization)을 함께 진행

- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거

- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듬.

- 규칙에 기반한 표기가 다른 단어들의 통합
    - 필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로, 같은 의미를 갖고 있음에도 푝가 다른 단어들을 하나의 단어로 정규화 하는 방법 사용.

- 대, 소문자 통합
    - 영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법.

    - 대문자와 소문자를 무작정 통합하는 것은 안된다.  
    대문자와 소문자가 구분 되어야 하는 경우도 있기 때문에.

    - 더 많은 변수를 사용해서 소문자 변환을 언제 사용할지 결정하는 머신 러닝 시퀀스 모델로 더 정확하게 진행 시킬 수 있다.

    - 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 함.

- 불필요한 단어의 제거
    - 정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자)을 의미하기도 하지만  
    분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 함.

    - 등장 빈도가 적은 단어
        - 텍스트 데이터에서 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존대
            - ex) 메일이 정상인지 스팸인지 분류하는 스팸 메일 분류기

    - 길이가 짧은 단어
        - 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당함.

        - 길이가 짧은 단어를 제거하는 2차 이유는 길이를 조건으로 텍스트를 삭제하면 단어가 아닌 구두점들까지도 한꺼번에 제거 할 수 있음.

        - 한국어에서는 길이가 짧은 단어라고 삭제하는 이런 방법은 크게 유효하지 않을 수 있다.
            - 한국어 단어는 한자어가 많고, 한 글자 만으로도 이미 의미를 가진 경우가 많다.

- 정규 표현식(Regular Expression)
    - 얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해 이를 제거할 수 있는 경우가 많다.

---
### 2-3 어간 추출(Stemming) and 표제어 추출(Lemmatization)

- 표제어 추출(Lemmatization)
    - 단어들로부터 표제어를 찾아가는 과정

    - 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단.

    - 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행

        ```
        1) 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분.
        2) 접사(affix) : 단어에 추가적인 의미를 주는 부분.
        ```

    - 형태학적 파싱은 위 두가지 구성 요소를 분리하는 작업.

    - NLTK의 표제어 추출을 위한 도구 WordNetLemmatizer가 있음.

- 어간 추출(Stemming)
    - 형태학적 분석을 단순화한 버전이라고 볼 수 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라 볼 수 있음.

    - 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수 있음.

    - 어간 추출 알고리즘 중 하나인 포터 알고리즘(Poter Algorithm) 사용

    - 규칙 기반의 접근을 하고 있으므로 어간 추출 후의 결과에는 사전에 없는 단어들도 포함되어 잇음.

    - 포터 알고리즘의 어간 추출은 여러개의 규칙을 가지고 있음
        ```
        ALIZE -> AL
        ANCE -> 제거
        ICAL -> IC

        위 규칙에 따르면 좌측의 단어는 우측의 단어와 같은 결과를 얻는다.
        formalize -> formal
        allowance -> allow
        electricical -> electric
        ```
    - 어간 추출 속도는 표제어 추출보다 일반적으로 빠름.

    - 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택

    - NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer)알고리즘을 지원

- 한국어에서의 어간 추출
    - 한국어는 5언 9품사의 구조를 가지고 있음.

        |언|품사|
        |---|---|
        |체언|명사, 대명사, 수사|
        |수식언|관형사, 부사|
        |관계언|조사|
        |독립언|감탄사|
        |용언|동사, 형용사|

    - 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성됨.

    ```
    1) 활용(conjugation) : 한국어에서만 가지는 특징이 아닌, 인도 유럽어(indo-european language)에서도 볼 수 있는 언어적 특징.
                           활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말함.
    
    어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(긋다, 긋고, 그어서, 그어라)

    어미(ending) : 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행

    2) 규칙 활용 : 어간이 어미를 취할 때, 어간의 모습이 일정하다.
        ex) 잡/어간 + 다/어미

    3) 불규칙 활용 : 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우.
    ```

---
### 2-4 불용어(Stopword)

- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서 큰 의미가 없는 단어 토큰을 제거하는 작업.

- 한국어에서 불용어 제거하기
    - 한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후 조사, 접속사 등을 제거하는 방법이 있음.

    - 사용자가 직접 불용어 사전을 만들어서 불용어를 제거하는 경우가 많음.

---
### 2-5 정규 표현식(Regular Expression)

- 정규 표현식 문법과 모듈 함수
    - 정규 표현식 모듈 re가 있음.

    - 텍스트 데이터를 빠르게 정게 할 수 있다.

    ```
    1) . 기호 : 한개의 임의의 문자를 나타냄.
        
    2) ? 기호 : ?앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있는 경우를 나타냄.

    3) * 기호 : 바로 앞의 문자가 0개 이상일 경우를 나타낸다.
                앞의 문자는 존재하지 않을 수도 있으며, 또는 여러 개일 수 있다.

    4) + 기호 : *와 유사함.
                앞의 문자가 최소 1개 이상이어야 한다.
                정규 표현식이 ab+c라고 한다면 ac는 매치되지 않는다.

    5) ^ 기호 : 시작되는 문자열을 지정한다.
                ^ab라면 문자열 ab로 시작되는 경우 매치

    6) {숫자} 기호 : 문자에 해당 기호를 붙이면, 해당 문자를 숫자만큼 반복한 것을 나타냄.
                    ab{2}c라면 a와 c 사이에 b가 존재하면서 b가 2개인 문자열에 대해서 매치

    7) {숫자1, 숫자2} 기호 : 문자에 해당 기호를 붙이면, 해당 문자를 숫자1 이상 숫자2 이하만큼 반복.
                            ab{2,8}c 라면 a와 c사이에 b가 존재하면서 b는 2개 이상 8이하인 문자열에 대해서 매치

    8) {숫자,} 기호 : 문자에 해당 기호를 붙이면 해당 문자를 숫자 이상 만큼 반복.
                      a{2,}bc라면 뒤에 bc가 붙으면서 a의 개수가 2개 이상인 경우인 문자열과 매치

    9) [] 기호 : []안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치.
                 [abc]라면 a 또는 b 또는 c가 들어있는 문자열과 매치.
                 범위지정도 가능([a-zA-Z]는 알파벳 전체를 의미, [0-9]는 숫자 전부를 의미)

    10) [^문자] 기호 : ^기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할
                      [^abc]라는 정규 표현식이 있다면 a 또는 b 또는 c가 들어간 문자열을 제외한 모든 문자열을 매치
    ```

    - 정규 표현식 모듈 함수
    ```
    1) re.match()와 re.search()의 차이 : search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, 
                                        match()는 문자열의 첫 부분부터 정규 표현식과 매치하는지를 확인
    
    2) re.split() : 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴.
                    토큰화에 유용하게 쓰일 수 있음.
                    공백을 기준으로 문자열 분리를 수행하고 결과로 리스트 리턴
    
    3) re.findall() : 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴.
                      매치되는 문자열이 없다면 빈 리스트를 리턴한다.
    
    4) re.sub() : 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있다.
                  정제 작업에 많이 사용.
                  영어 문장에 각주 등과 같은 이유로 특수 문자가 섞여있는 경우에 특수 문자를 제거하고 싶다면 알파벳 외의 문자는 
                  공백으로 처리하는 용도로 사용 가능.
    ```

---
### 2-6 정수 인코딩(Integer Encoding)

- 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법이 있음.

- 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 정수에 맵핑(mapping)시키는 전처리 작업이 필요.

- 정수 인코딩
    - 단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고,  
    빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있음.