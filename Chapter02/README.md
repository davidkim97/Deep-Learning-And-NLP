# Chapter 02. 텍스트 전처리(Text preprocessing)

### 2-1 토큰화(Tokenization)

- 코퍼스 테이터가 필요에 맞게 전처리 되어 있지 않은 상태라면, 용도에 맞게 토큰화(Tokenization), 정제(cleaning), 정규화(normalization)을 진행

- 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 한다.

- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의

- 단어 토큰화(Word Tokenization)

    - 토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화(word tokenization)라고 한다.

    - 단어 외에도 단어구, 의미를 갖는 문자열로 간주되기도 함.

    - 토큰화 작업은 단순히 구두점이나 특수문자를 전부 제거하는 정제(cleaning)작업을 수행하는 것만으로 해결되지 않음.

    - 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우가 발생

    - 띄어쓰기 단위로 자르면 사실상 단어 토큰이 구분되는 영어와 달리 한국어는 띄어쓰기만으로 단어 토큰을 구분하기 어려움.

- 토큰화 중 생기는 선택의 순간

    - 영어의 경우 NLTK의 word_tokenize와 WordPunctTokenizer를 사용해서 어퍼스트로피를 처리할 수 있다.

- 토큰화에서 고려해야할 사항
    - 구두점이나 특수 문자를 단순 제외해서는 안된다.
        - 문장의 마침표의 경우 문장의 경계를 알 수 있는데 도움이 되므로 단어를 뽑아낼 때 마침표를 제외하지 않을 수 있다.

        - 단어 자체에 구두점을 갖고 있는 경우도 있다.(Ph.D, AT&T) 또 특수 문자의 달러나 슬래시

        - 숫자 사이에 콤마가 들어가 있는 경우(수치를 표현할때 세 자리 단위로 콤마)

    - 줄임말과 단어 내에 띄어쓰기가 있는 경우
        - 단어가 줄임말로 쓰일 때 생기는 형태 (I am -> I'm(m을 접어라고 함.))

        - New York이라는 단어나 rock 'n' roll을 보면 하나의 단어이지만 중간에 띄어쓰기가 존재  
        -> 사용 용도에 따라서 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야 할 경우도 있으므로 토큰화 작업은 저런 단어를 하나로 인식 할 수 있는 능력을 가지고 있어야 함.

    - 표준 토큰화 예제
        - 표준으로 쓰이고 있는 토큰화 방법 중 하나인 'Penn Treebank Tokenization'

        ```
        규칙 1. 하이픈으로 구성된 단어는 하나로 유지
        규칙 2. dosen't와 같이 어퍼스트로피로 '접어'가 함께하는 단어는 분리
        ```

- 문장 토큰화(Sentence Tokenization)
    - 코퍼스 내에서 문장 단위로 구분하는 작업으로 때로는 문장 분류(sentence segmentation)라고 부른다.

    - 예시
    ```
    EX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.

    EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.
    ```

    - 사용하는 코퍼스가 어떤 국적의 언어인지, 또는 해당 코퍼스 내에서 특수문자들이 어떻게 사용되고 있는지에 따라서 직접 규칙들을 정의 해 볼 수 있음.

    - NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원

    - NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않음.

    - 한국어에 대한 문장 토큰화 도구 또한 존재 KSS(Korean Sentence Splitter)

- 한국어에서의 토큰화의 어려움
    - 한국어는 영어와 달리 띄어쓰기로 토큰화를 하기 부족

    - 어절 토큰화는 한국어 NLP에서 지양되고 있다.

    - 한국어가 영어와는 다른 형태를 가지는 언어인 교착어라는 점에서 어려운 점이 있음,

    - 교착어의 특성
        - 띄어쓰기 단위가 영어처럼 독립적인 단어라면 띄어쓰기 단위로 토큰화를 하면 되겠지만 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라  
        조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리 해줘야 한다.

        - 한국어 토큰화에서는 형태소(morpheme)란 개념을 이해 해야한다.

        - 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말한다.

        - 형태소의 종류
            - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자제촐 단어가 된다.  
            체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.

            - 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간을 말한다.

    - 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.

        - 한국어 코퍼스가 뉴스 기사와 같이 띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우에 띄어쓰기가 틀렸거나 지켜지지 않는  
        코퍼스가 많이 존재

- 품사 태깅(Part-of-speech tagging)

    - 단어는 표기는 같지만 품사에 따라서 단어의 의미가 달라지기도 한다.

    - 그에 따라 단어 토큰화 과정에서 각 단어가 어떤 품사골 쓰였는지를 구분해 놓기도 하는데, 이 작업을 품사 태깅(part-of-speech tagging)이라고 한다.

    - NLTK와 KoNLPy를 통해 품사 태깅을 할 수 있다.

    - 영어 문장에서 토큰화 결과에 품사 태깅을 진행하면 PennTreebank POG Tags에서

    |명칭|설명|
    |---|---|
    |PRP|인칭 대명사|
    |VBP|동사|
    |RB|부사|
    |VBG|현재부사|
    |IN|전치사|
    |NNP|고유 명사|
    |NNS|복수형 명사|
    |CC|접속사|
    |DT|관사|

    - 한국어 형태소 토큰화의 경우 
    ```
    1) morphs : 형태소 추출
    2) pos : 품사 태깅(Part-of-speech tagging)
    3) nouns : 명사 추출
    ```

    - 각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 따라 판단하고 사용.

---
### 2-2 정제(Cleaning) and 정규화(Normalization)
- 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(Tokenization)라고 하며, 데이터를 용도에 맞게 정제(cleaning)및 정규화(normalization)을 함께 진행

- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거

- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듬.

- 규칙에 기반한 표기가 다른 단어들의 통합
    - 필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로, 같은 의미를 갖고 있음에도 푝가 다른 단어들을 하나의 단어로 정규화 하는 방법 사용.

- 대, 소문자 통합
    - 영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법.

    - 대문자와 소문자를 무작정 통합하는 것은 안된다.  
    대문자와 소문자가 구분 되어야 하는 경우도 있기 때문에.

    - 더 많은 변수를 사용해서 소문자 변환을 언제 사용할지 결정하는 머신 러닝 시퀀스 모델로 더 정확하게 진행 시킬 수 있다.

    - 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 함.

- 불필요한 단어의 제거
    - 정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자)을 의미하기도 하지만  
    분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 함.

    - 등장 빈도가 적은 단어
        - 텍스트 데이터에서 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존대
            - ex) 메일이 정상인지 스팸인지 분류하는 스팸 메일 분류기

    - 길이가 짧은 단어
        - 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당함.

        - 길이가 짧은 단어를 제거하는 2차 이유는 길이를 조건으로 텍스트를 삭제하면 단어가 아닌 구두점들까지도 한꺼번에 제거 할 수 있음.

        - 한국어에서는 길이가 짧은 단어라고 삭제하는 이런 방법은 크게 유효하지 않을 수 있다.
            - 한국어 단어는 한자어가 많고, 한 글자 만으로도 이미 의미를 가진 경우가 많다.

- 정규 표현식(Regular Expression)
    - 얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해 이를 제거할 수 있는 경우가 많다.

---
### 2-3 어간 추출(Stemming) and 표제어 추출(Lemmatization)

- 표제어 추출(Lemmatization)
    - 단어들로부터 표제어를 찾아가는 과정

    - 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단.

    - 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행

        ```
        1) 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분.
        2) 접사(affix) : 단어에 추가적인 의미를 주는 부분.
        ```

    - 형태학적 파싱은 위 두가지 구성 요소를 분리하는 작업.

    - NLTK의 표제어 추출을 위한 도구 WordNetLemmatizer가 있음.

- 어간 추출(Stemming)
    - 형태학적 분석을 단순화한 버전이라고 볼 수 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라 볼 수 있음.

    - 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수 있음.

    - 어간 추출 알고리즘 중 하나인 포터 알고리즘(Poter Algorithm) 사용

    - 규칙 기반의 접근을 하고 있으므로 어간 추출 후의 결과에는 사전에 없는 단어들도 포함되어 잇음.

    - 포터 알고리즘의 어간 추출은 여러개의 규칙을 가지고 있음
        ```
        ALIZE -> AL
        ANCE -> 제거
        ICAL -> IC

        위 규칙에 따르면 좌측의 단어는 우측의 단어와 같은 결과를 얻는다.
        formalize -> formal
        allowance -> allow
        electricical -> electric
        ```
    - 어간 추출 속도는 표제어 추출보다 일반적으로 빠름.

    - 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택

    - NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer)알고리즘을 지원

- 한국어에서의 어간 추출
    - 한국어는 5언 9품사의 구조를 가지고 있음.

        |언|품사|
        |---|---|
        |체언|명사, 대명사, 수사|
        |수식언|관형사, 부사|
        |관계언|조사|
        |독립언|감탄사|
        |용언|동사, 형용사|

    - 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성됨.

    ```
    1) 활용(conjugation) : 한국어에서만 가지는 특징이 아닌, 인도 유럽어(indo-european language)에서도 볼 수 있는 언어적 특징.
                           활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말함.
    
    어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(긋다, 긋고, 그어서, 그어라)

    어미(ending) : 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행

    2) 규칙 활용 : 어간이 어미를 취할 때, 어간의 모습이 일정하다.
        ex) 잡/어간 + 다/어미

    3) 불규칙 활용 : 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우.
    ```

---
### 2-4 불용어(Stopword)

- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서 큰 의미가 없는 단어 토큰을 제거하는 작업.

- 한국어에서 불용어 제거하기
    - 한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후 조사, 접속사 등을 제거하는 방법이 있음.

    - 사용자가 직접 불용어 사전을 만들어서 불용어를 제거하는 경우가 많음.

---
### 2-5 정규 표현식(Regular Expression)

- 정규 표현식 문법과 모듈 함수
    - 정규 표현식 모듈 re가 있음.

    - 텍스트 데이터를 빠르게 정게 할 수 있다.

    ```
    1) . 기호 : 한개의 임의의 문자를 나타냄.
        
    2) ? 기호 : ?앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있는 경우를 나타냄.

    3) * 기호 : 바로 앞의 문자가 0개 이상일 경우를 나타낸다.
                앞의 문자는 존재하지 않을 수도 있으며, 또는 여러 개일 수 있다.

    4) + 기호 : *와 유사함.
                앞의 문자가 최소 1개 이상이어야 한다.
                정규 표현식이 ab+c라고 한다면 ac는 매치되지 않는다.

    5) ^ 기호 : 시작되는 문자열을 지정한다.
                ^ab라면 문자열 ab로 시작되는 경우 매치

    6) {숫자} 기호 : 문자에 해당 기호를 붙이면, 해당 문자를 숫자만큼 반복한 것을 나타냄.
                    ab{2}c라면 a와 c 사이에 b가 존재하면서 b가 2개인 문자열에 대해서 매치

    7) {숫자1, 숫자2} 기호 : 문자에 해당 기호를 붙이면, 해당 문자를 숫자1 이상 숫자2 이하만큼 반복.
                            ab{2,8}c 라면 a와 c사이에 b가 존재하면서 b는 2개 이상 8이하인 문자열에 대해서 매치

    8) {숫자,} 기호 : 문자에 해당 기호를 붙이면 해당 문자를 숫자 이상 만큼 반복.
                      a{2,}bc라면 뒤에 bc가 붙으면서 a의 개수가 2개 이상인 경우인 문자열과 매치

    9) [] 기호 : []안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치.
                 [abc]라면 a 또는 b 또는 c가 들어있는 문자열과 매치.
                 범위지정도 가능([a-zA-Z]는 알파벳 전체를 의미, [0-9]는 숫자 전부를 의미)

    10) [^문자] 기호 : ^기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할
                      [^abc]라는 정규 표현식이 있다면 a 또는 b 또는 c가 들어간 문자열을 제외한 모든 문자열을 매치
    ```

    - 정규 표현식 모듈 함수
    ```
    1) re.match()와 re.search()의 차이 : search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, 
                                        match()는 문자열의 첫 부분부터 정규 표현식과 매치하는지를 확인
    
    2) re.split() : 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴.
                    토큰화에 유용하게 쓰일 수 있음.
                    공백을 기준으로 문자열 분리를 수행하고 결과로 리스트 리턴
    
    3) re.findall() : 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴.
                      매치되는 문자열이 없다면 빈 리스트를 리턴한다.
    
    4) re.sub() : 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있다.
                  정제 작업에 많이 사용.
                  영어 문장에 각주 등과 같은 이유로 특수 문자가 섞여있는 경우에 특수 문자를 제거하고 싶다면 알파벳 외의 문자는 
                  공백으로 처리하는 용도로 사용 가능.
    ```

---
### 2-6 정수 인코딩(Integer Encoding)

- 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법이 있음.

- 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 정수에 맵핑(mapping)시키는 전처리 작업이 필요.

- 정수 인코딩
    - 단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고,  
    빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있음.

---

### 2-7 패딩(Padding)

- 자연어 처리를 하다 보면 각 문장(또는 문서)은 서로 길이가 다를 수 있다.  
-> 이 것을 정리하는 것을 Padding이라고 한다.

- Numpy로 패딩하기
    ```python
    import numpy as np
    from tesorflow.keras.preprocessing.text import Tokenizer
    ```

    - 단어 집합을 만들고 정수 인코딩을 수행 한 다음 진행.

- 케라스 전처리 도구로 패딩하기
    ```python
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    ```

---

### 2-8 원-핫 인코딩(One-Hot Encoding)
- 자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법들이 있는데, 원-핫 인코딩(One-Hot Encoding)은 많으 기법중 기본적인 표현 방법.

- 단어집합(Vocabulary) : 서로 다른 단어들의 집합

- 원-핫 인코딩을 위해서는 먼저 단어 집합을 만들어야 한다.

- 원-핫 인코딩(One-Hot Encoding) 이란?
    - 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고싶은 단어의 인덱스에 1의 값을 부여, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식

    - 원-핫 인코딩을 두 가지 과정으로 정리
        ```
        1. 정수 인코딩을 수행(단어에 고유한 정수 부여)
        2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여, 다른 던어의 인덱스의 위치에는 0을 부여
        ```
    
    ```python
    # 문장 : 나는 자연어 처리를 배운다.
    # Okt 형태소 분석기를 통해서 문장 토큰화 수행
    from konlpy.tag import Okt  

    okt = Okt()  
    tokens = okt.morphs("나는 자연어 처리를 배운다")  
    print(tokens)
    ```
- 케라스(Keras)를 이용한 원-핫 인코딩(One-Hot Encoding)
    - 케라스 원-핫 인코딩을 수행하는 유용한 도구 to_categorical()를 지원

- 원-핫 인코딩(One-Hot Encoding)의 한계
    - 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점.  
    -> 다른 표현으로 벡터의 차원이 늘어난다고 표현

    - 단어의 유사도를 표현하지 못한다.

    - 단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서 문제가 될 소지가 있음.

    - 이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지 있음.
    ```
    1. 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있다.
    2. 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText등이 있다.
    3. 그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 존재
    ```

---
### 2-9 데이터 분리(Splitting Data)
- 머신 러닝 모델을 학습시키고 평가하기 위해서는 데이터를 적절하게 분리하는 작업이 필요

- 지도 학습(Supervised Learning)을 사용하여 데이터 분리 작업 실행.

- 지도 학습(Supervised Learning)
    - 지도 학습의 훈련 데이터는 문제지를 연상케 한다.

    - 지도 학습의 훈련 데이터는 정답이 무엇인지 맞춰 하는 '문제'에 해당되는 데이터와 레이블이라 부르는 '정답'이 적혀있는 데이터로 구성.

    - 스팸 분류기를 통한 예시
        ```
        기계를 훈련시키기 위해 데이터를 총 4개로 나눈다.
        우선 메일의 내용이 담긴 첫번째 열을 X에 저장.
        그리고 메일이 스팸인지 정상인지 정답이 적혀있는 두번째 열을 y에 저장.
        이 문제지에 해당되는 20,000개의 X와 정답지에 해당되는 20,000개의 y가 생성됨.

        이제 X와 y에 대해서 일부 데이터를 또 다시 분리.
        이는 문제지를 다 공부하고나서 실력을 평가하기 위해서 시험(test)용으로 일부로 일부 문제와 해당 문제지의 정답지를 분리해놓는 것이다.
        분리시 여전히 X와 y의 맵핑 관계를 유지해야 한다.
        어떤 X(문제)에 대한 어떤 y(정답)인지 바로 찾을 수 있어야 한다.

        <훈련 데이터>
        X_train : 문제지 데이터
        y_train : 문제지에 대한 정답 데이터.

        <테스트 데이터>
        X_test : 시험지 데이터
        y_test : 시험지에 대한 정답 데이터
        ```


- X와 y 분리하기
    - zip 함수를 이용하여 분리

        - zip() 함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할.
        
        - 리스트의 리스트구성에서 zip 함수는 X와 y를 분리하는데 유용하다.

- 테스트 데이터 분리하기
    - 사이킷 런을 이용하여 분리

    - 사이킷런은 학습용 테스트와 테스트용 데이터를 쉽게 분리할 수 있게 해주는 train_test_split()을 지원

        ```python
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)
        ```

        - train_size와 test_size는 둘 중 하나만 기재해도 된다.

        - X : 독립 변수 데이터(배열이나 데이터프레임)

        - y : 종속 변수 데이터, 레이블 데이터

        - test_size : 테스트용 데이터 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.

        - train_size : 학습용 데이터의 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.

        - random_state : 난수 시드

    - 수동으로 분리하기

---

### 2-10 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)

- PyKoSpacing
    - 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지.

    - 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있음.

    ```python
    pip install git+https://github.com/haven-jeon/PyKoSpacing.git
    ```

- Py-Hanspell
    - 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지
    ```python
    pip install git+https://github.com/ssut/py-hanspell.git

    from hanspell import spell_checker
    ```

- SOYNLP를 이용한 단어 토큰화
    - soynlp는 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저

    - 비지도 학습으로 단어 토큰화를 한다는 특징을 가지고 있으며, 데이터에 자주 등장하는 단어들을 단어로 분석

    - soynlp 단어 토크나이저는 내부적으로 단어 점수 표로 동작.

    - 이 점수는 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용

    ```python
    pip install soynlp
    ```

    - 신조어 문제
        - 기존의 형태소 분석기는 신조어나 형태소 분석기에 등록되지 않은 단어 같은 경우에는 제대로 구분하지 못하는 단점이 있음.

    - 학습하기
        - soynlp는 학습 기반의 단어 토크나이저이므로 기존의 KoNLPy에서 제공하는 형태소 분석기들과는 달리 학습 과정을 거쳐야 한다.

        - 전체 코퍼스로부터 응집 확률과 브랜칭 엔트로피 단어 점수표를 만드는 과정이다.

        - WordExtractor.extract()를 통해 전체 코퍼스에 대해 단어 점수표 계산

    - SOYNLP의 응집 확률(cohesion probability)
        - 응집 확률은 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지 판단하는 척도이다.

        - 응집 확률은 문자열을 문자 단위로 분리하여 내부 문자열을 만드는 과정에서 왼쪽부터 순서대로 문자를 추가하면서  
        각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산하여 누적곱을 한 값.

        - 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높다.

    - SOYNLP의 브랜칭 엔트로피(branching entropy)
        - 확률 분포의 엔트로피값을 사용.

        - 주어진 문자열에서 얼마나 다음 문자가 등장할 수 있는지를 판단하는 척도이다.

    - SOYNLP의 L tokenizer
        - 한국어는 띄어쓰기 단위로 나눈 어절 토큰은 주로 L 토큰 + R 토큰의 형식을 가질 때가 많다.

        - L 토크나이저는 L 토큰 + R 토큰으로 나누되, 분리 기준을 점수가 가장 높은 L 토큰을 찾아내는 원리를 가지고 있다.

    - 최대 점수 토크나이저
        - 띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저.

- SOYNLP를 이용한 반복되는 문자 정제
    - SNS나 채팅 데이터와 같은 한국어 데이터의 경우에는 ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우가 많은데 반복되는 것은 하나로 정규화를 시켜 준다.

- Customized KoNLPy
    - 영어권 언어는 띄어쓰기만해도 단어들이 잘 분리되지만, 한국어는 그렇지 않다.

    - 한국어 데이터를 사용해여 모델을 구현하는 것만큼 형태소 분석기를 사용해서 단어 토큰화 진행

    - 사용자 사전을 추가하는 방법은 형태소 분석기마다 다른데, 생각보다 복잡한 경우들이 많다.

    - Customized Konlpy라는 사용자 사전 추가가 매우 쉬운 패키지가 있다.
        ```python
        !pip install customized_konlpy
        ```